{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in /home/deptinfo/.local/lib/python3.8/site-packages (1.19.4)\n",
      "Requirement already up-to-date: nltk in /home/deptinfo/.local/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: regex in /home/deptinfo/.local/lib/python3.8/site-packages (from nltk) (2020.9.27)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/deptinfo/.local/lib/python3.8/site-packages (from nltk) (4.50.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/deptinfo/.local/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: pandas in /home/deptinfo/.local/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/deptinfo/.local/lib/python3.8/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Installation et Import des packages necessaires (pandas, Nltk)\n",
    "!pip3 install --user -U numpy\n",
    "!pip3 install --user -U nltk\n",
    "!pip3 install pandas\n",
    "\n",
    "## Import de packages necessaires pour la suite\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as et \n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import difflib\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "lem = WordNetLemmatizer() \n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraction des données\n",
    "#suppression des données inutiles (ne comportant pas d'aspect terms)\n",
    "#Suppresion des polarité conflictuelle\n",
    "def xmltodata(xml):\n",
    "    df = et.parse(xml)\n",
    "    data  = df.getroot()\n",
    "\n",
    "    texte = []\n",
    "    for texts in data:\n",
    "        term = []\n",
    "        for text in texts:\n",
    "            if text.tag == \"text\" :\n",
    "                term.append(text.text)\n",
    "            elif text.tag == \"aspectTerms\":\n",
    "                for at in text :\n",
    "                    for pol,val in at.attrib.items() :\n",
    "                        if pol == 'polarity' :\n",
    "                            if val != \"conflict\":\n",
    "                                term.append(at.attrib)\n",
    "        if len(term)>1:\n",
    "            texte.append(term)\n",
    "    return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récuperation des mots negatifs et positifs de la BD \"Sentiment Lexicon \" sur Kaggle\n",
    "df1 = pd.read_fwf('negative_words_en.txt', header=None, index=False)\n",
    "negatif = df1.values.tolist()\n",
    "df2 = pd.read_fwf('positive_words_en.txt', header=None, index=False)\n",
    "positif = df2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On simplifie le dictionnaire de chaque commentaire en transformant la val\n",
    "#de polarity avec des chiffres\n",
    "#negatif = -1 ; neutre = 0 ; positif = 1\n",
    "def simple_polarity(data):\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])-1):\n",
    "            if data[i][j+1]['polarity'] == \"negative\":\n",
    "                data[i][j+1]['polarity'] = -1\n",
    "            elif data[i][j+1]['polarity'] == \"neutral\":\n",
    "                data[i][j+1]['polarity'] = 0\n",
    "            elif data[i][j+1]['polarity'] == \"positive\":\n",
    "                data[i][j+1]['polarity'] = 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retire les stop_word\n",
    "def no_stopword(word):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    token = nltk.word_tokenize((word.lower()))\n",
    "    tagged = nltk.pos_tag(token)\n",
    "    filtered_sentence = []\n",
    "    for w in token:  \n",
    "        if w not in stop_words:  \n",
    "            filtered_sentence.append(w) \n",
    "    return filtered_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmentation de la phrase pour chaque aspect terme en fonction de la ponctuation ou de sa position dans la phrase\n",
    "def flag_text(texte):\n",
    "    text = texte[0]\n",
    "    aspects_terms = []\n",
    "    for i in range(len(texte)-1):\n",
    "        aspects_terms.append(texte[i+1]['term'])\n",
    "    sentences = re.split(r' *[\\.\\?!,][\\'\"\\)\\]]* *', text)\n",
    "    if \"\" in sentences :\n",
    "        sentences.remove(\"\")    \n",
    "    if len(sentences) > 1 :\n",
    "        dict_terms_sentences = {curr_aspect : [curr_sentence for curr_sentence in sentences \n",
    "                                                                if curr_aspect.lower() in curr_sentence.lower()] \n",
    "                                                                for curr_aspect in aspects_terms}\n",
    "    elif len(aspects_terms) > 1 : \n",
    "        for term in aspects_terms :\n",
    "            textsplit = []\n",
    "            split = text.split(term)           \n",
    "            if len(textsplit) < 1:\n",
    "                textsplit.append(split[0] + \" \" + term)\n",
    "            else :\n",
    "                textsplit.append(split[1]+ \" \" + term)        \n",
    "        dict_terms_sentences = {curr_aspect : [curr_sentence for curr_sentence in textsplit\n",
    "                                                                if curr_aspect.lower() in curr_sentence.lower()]\n",
    "                                                                for curr_aspect in aspects_terms}\n",
    "    else :     \n",
    "        dict_terms_sentences = { aspects_terms[0] : [text]}\n",
    "    \n",
    "    return dict_terms_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des Keywords qui sont susceptibles d'avoir un lien\n",
    "#Avec le sentiment\n",
    "#EN prenant en compte la possible eventualité que nous ne trouvons pas de keywords (donc mot neutre)\n",
    "def keywords_aspect(dic):\n",
    "    keyWord = []\n",
    "    neg = []\n",
    "    for words in dic:\n",
    "        text = dic[words]\n",
    "        if len(dic[words])>0:\n",
    "            aspect = words\n",
    "            token = nltk.word_tokenize((text[0].lower()))\n",
    "            tagged = nltk.pos_tag(token)\n",
    "            aspectWord = []\n",
    "            for i in range(len(tagged)):\n",
    "                if tagged[i][1] in ['RP','JJ','RB','NN']:\n",
    "                    aspectWord.append(tagged[i])\n",
    "            if aspectWord == [] :\n",
    "                aspectWord.append(\"\")\n",
    "            keyWord.append(aspectWord)\n",
    "        else :\n",
    "            keyWord.append([\"\"])\n",
    "    return keyWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des Keywords qui sont susceptibles d'avoir un lien\n",
    "#Avec le sentiment de l'aspect en prenant en compte les \"not et n't avant le mot\"\n",
    "#C'est la version utilisé\n",
    "def keywords_aspect_not(dic):\n",
    "    keyWord = []\n",
    "    neg = []\n",
    "    for words in dic:\n",
    "        text = dic[words]\n",
    "        if len(dic[words])>0:\n",
    "            aspect = words\n",
    "            token = nltk.word_tokenize((text[0].lower()))\n",
    "            tagged = nltk.pos_tag(token)\n",
    "            aspectWord = []\n",
    "            for i in range(len(tagged)):\n",
    "                if tagged[i][1] in ['RP','JJ','RB','NN']:\n",
    "                    if i == 0 :\n",
    "                        aspectWord.append([\"p\",tagged[i]])\n",
    "                    if i > 0:\n",
    "                        if tagged[i-1][0] == \"not\" or tagged[i-1][0] ==\"n't\":\n",
    "                            aspectWord.append([\"n\",tagged[i]])\n",
    "                        else :\n",
    "                            aspectWord.append([\"p\",tagged[i]])\n",
    "            if aspectWord == [] :\n",
    "                aspectWord.append([\"p\",\"\"])\n",
    "            keyWord.append(aspectWord)\n",
    "        else :\n",
    "            keyWord.append([[\"p\",\"\"]])\n",
    "    return keyWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui applique keywords_aspect() a toute la data\n",
    "def keywords_data_not(data):\n",
    "    keywords = []\n",
    "    for i in range(len(data)):\n",
    "        keywords.append(keywords_aspect_not(flag_text(data[i])))\n",
    "    return keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui applique keywords_aspect_not() a toute la data\n",
    "#C'est la version utilisé\n",
    "def keywords_data(data):\n",
    "    keywords = []\n",
    "    for i in range(len(data)):\n",
    "        keywords.append(keywords_aspect(flag_text(data[i])))\n",
    "    return keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui va calculer la polarité en comparant les keywords avec \n",
    "#les données positifs/negatifs récupéré sur kaggle\n",
    "def com_polarity(data):\n",
    "    datalist = []\n",
    "    for com in data :\n",
    "        comlist =  []\n",
    "        for aspect in com:\n",
    "            aspectlist = []\n",
    "            for tagged in aspect :\n",
    "                if len(tagged) != 0 :\n",
    "                    word = lem.lemmatize(tagged[0])\n",
    "                else :\n",
    "                    word = \" \"\n",
    "                nbneg = 0\n",
    "                nbpos = 0\n",
    "                for mot_neg in negatif:\n",
    "                    if word == mot_neg[0]:\n",
    "                        nbneg += 1\n",
    "                for mot_pos in positif:\n",
    "                    if word == mot_pos[0]:\n",
    "                        nbpos += 1       \n",
    "                if nbneg>nbpos :\n",
    "                    aspectlist.append(-1)\n",
    "                elif nbneg<nbpos:\n",
    "                    aspectlist.append(1)\n",
    "                elif nbneg == nbpos:\n",
    "                    aspectlist.append(0)\n",
    "            if len(aspectlist) < 1 :\n",
    "                aspectlist.append(0)\n",
    "            somme = sum(aspectlist)\n",
    "            if somme > 0 :\n",
    "                comlist.append(1)\n",
    "            elif somme < 0 :\n",
    "                comlist.append(-1)\n",
    "            else :               \n",
    "                comlist.append(0)\n",
    "                \n",
    "        datalist.append(comlist)\n",
    "    return datalist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cette version prends en compte les \"not\" et \"n't\"\n",
    "#Fonction qui va calculer la polarité en comparant les keywords avec \n",
    "#les données positifs/negatifs récupéré sur kaggle\n",
    "#C'est la version utilisé\n",
    "def com_polarity_not(data):\n",
    "    datalist = []\n",
    "    for com in data :\n",
    "        #print(com)\n",
    "        comlist =  []\n",
    "        for aspect in com:\n",
    "            #print(aspect)\n",
    "            aspectlist = []\n",
    "            for tagged in aspect :\n",
    "                if len(tagged[1]) != 0 :\n",
    "                    word = lem.lemmatize(tagged[1][0])\n",
    "                else :\n",
    "                    word = \" \"\n",
    "                nbneg = 0\n",
    "                nbpos = 0\n",
    "                if tagged[0] == 'p' :\n",
    "                    for mot_neg in negatif:\n",
    "                        if word == mot_neg[0]:\n",
    "                            nbneg += 1\n",
    "                    for mot_pos in positif:\n",
    "                        if word == mot_pos[0]:\n",
    "                            nbpos += 1  \n",
    "                elif tagged[0] == 'n':\n",
    "                    for mot_neg in negatif:\n",
    "                        if word == mot_neg[0]:\n",
    "                            nbpos += 1\n",
    "                    for mot_pos in positif:\n",
    "                        if word == mot_pos[0]:\n",
    "                            nbneg += 1 \n",
    "                    \n",
    "                if nbneg>nbpos :\n",
    "                    aspectlist.append(-1)\n",
    "                elif nbneg<nbpos:\n",
    "                    aspectlist.append(1)\n",
    "                elif nbneg == nbpos:\n",
    "                    aspectlist.append(0)\n",
    "            if len(aspectlist) < 1 :\n",
    "                aspectlist.append(0)\n",
    "            somme = sum(aspectlist)\n",
    "            if somme > 0 :\n",
    "                comlist.append(1)\n",
    "            elif somme < 0 :\n",
    "                comlist.append(-1)\n",
    "            else :               \n",
    "                comlist.append(0)\n",
    "                \n",
    "        datalist.append(comlist)\n",
    "    return datalist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumere le nombre de positif, neutre et negatif par phrase (reponse)\n",
    "#Non utilisé\n",
    "def numaspect(data):\n",
    "    poldata = []\n",
    "    for j in range(len(data)):\n",
    "        neg = 0\n",
    "        pos = 0\n",
    "        neu = 0\n",
    "        for i in range(len(data[j])-1):\n",
    "            if data[j][i+1]['polarity'] == -1:\n",
    "                neg += 1\n",
    "            elif data[j][i+1]['polarity'] == 1:\n",
    "                pos += 1\n",
    "            else :\n",
    "                neu += 1\n",
    "        poldata.append([neg,pos,neu])            \n",
    "    return poldata      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enumere la polarité des terms (reponses)\n",
    "def pol_doc_aspect(data):\n",
    "    res = []\n",
    "    for com in data:\n",
    "        polaspect = []\n",
    "        for i in range(len(com)-1):\n",
    "            polaspect.append(com[i+1]['polarity'])\n",
    "        res.append(polaspect)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ce sont les fcts qui vont appliaquer toutes les fct précédente a la data\n",
    "def preparation_data(fichier) :\n",
    "    data_train = xmltodata(fichier)\n",
    "    data_train_simple = simple_polarity(data_train)\n",
    "    data_keywords = keywords_data_not(data_train_simple)\n",
    "    pol_asp_data = com_polarity_not(data_keywords)\n",
    "    \n",
    "    return pol_asp_data\n",
    "\n",
    "def preparation_rep(fichier):\n",
    "    data_test= xmltodata(fichier)\n",
    "    data_simple = simple_polarity(data_test)\n",
    "    #data_test_pol = numaspect(data_test)\n",
    "    data_val = pol_doc_aspect(data_simple)\n",
    "    return data_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLICATION DES FONCTIONS SUR LES DATAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resto_train = preparation_data(\"Restaurants_Train.xml\")\n",
    "res_resto_train = preparation_rep(\"Restaurants_Train.xml\")\n",
    "\n",
    "data_resto_test = preparation_data(\"Restaurants_Test_Gold.xml\")\n",
    "res_resto_test = preparation_rep(\"Restaurants_Test_Gold.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_laptop_train = preparation_data(\"Laptop_Train.xml\")\n",
    "res_laptop_train = preparation_rep(\"Laptop_Train.xml\")\n",
    "\n",
    "data_laptop_test = preparation_data(\"Laptop_Test_Gold.xml\")\n",
    "res_laptop_test = preparation_rep(\"Laptop_Test_Gold.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_laptop_nolabel = preparation_data(\"Laptop_Test_NoLabels.xml\")\n",
    "data_Restaurants_nolabel = preparation_data(\"Restaurants_Test_NoLabels.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(data_res,test_data) :   \n",
    "    repjuste = 0\n",
    "    repfaus = 0\n",
    "    totalinspectid = 0\n",
    "    totalinspectrep = 0\n",
    "    for i in range(len(data_res)):\n",
    "        for k in range(len(data_res[i])):\n",
    "            totalinspectrep += 1\n",
    "        for j in range(len(test_data[i])):\n",
    "            if test_data[i][j] == data_res[i][j]:\n",
    "                repjuste += 1\n",
    "                totalinspectid +=1\n",
    "            elif test_data[i][j] != data_res[i][j]:\n",
    "                repfaus +=1\n",
    "                totalinspectid +=1\n",
    "                \n",
    "                \n",
    "    precision = repjuste/totalinspectid\n",
    "    rappel = repjuste/totalinspectrep\n",
    "    print(\"precision : \" ,precision)\n",
    "    print(\"rappel : \",rappel)\n",
    "    print(\"Resultat final : \", 2*((precision*rappel)/(precision+rappel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  0.5326421966937518\n",
      "rappel :  0.5277623542476402\n",
      "Resultat final :  0.5301910472737414\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation sur le dataset : Restaurants_Train.xml\")\n",
    "evaluation(res_resto_train,data_resto_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  0.6979166666666666\n",
      "rappel :  0.6979166666666666\n",
      "Resultat final :  0.6979166666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation sur le dataset : Restaurants_Test_Gold.xml\")\n",
    "evaluation(res_resto_test,data_resto_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  0.4897959183673469\n",
      "rappel :  0.4897959183673469\n",
      "Resultat final :  0.4897959183673469\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation sur le dataset : Laptop_Train.xml\")\n",
    "evaluation(res_laptop_test,data_laptop_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  0.48610498456109397\n",
      "rappel :  0.4764375270211846\n",
      "Resultat final :  0.4812227074235808\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation sur le dataset : Laptop_Test_Gold.xml\")\n",
    "evaluation(res_laptop_train,data_laptop_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
